<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>WebRTC VAD WebAssembly Demo - Strict Silence Stop</title>
  <style>
    body { font-family: sans-serif; padding: 20px; }
    #log { white-space: pre-wrap; border: 1px solid #ccc; padding: 10px; height: 200px; overflow-y: scroll; }
    button { margin: 5px; padding: 10px 20px; }
  </style>
</head>
<body>

<h1>WebRTC VAD Demo</h1>

<button id="startBtn">Start VAD</button>
<button id="stopBtn" disabled>Stop VAD</button>

<div id="log">Press Start to begin</div>
<script src="./vad.js"></script>

<script>
(async () => {
  const logEl = document.getElementById('log');
  const startBtn = document.getElementById('startBtn');
  const stopBtn = document.getElementById('stopBtn');
  function log(msg) {
    logEl.textContent += msg + '\n';
    logEl.scrollTop = logEl.scrollHeight;
    console.log(msg);
  }
  const SAMPLE_RATE = 16000;
  const FRAME_DURATION_MS = 20;
  const FRAME_SAMPLES = SAMPLE_RATE * FRAME_DURATION_MS / 1000; // 320 samples per frame
  const SILENCE_FRAME_COUNT = 30; 
  let silenceFrames = 0;
  let stopped = true;
  let Module;
  let vadInstancePtr;
  let audioBufferPtr;
  let audioContext;
  let processorNode;
  let micStream;
  let floatBuffer = [];

  function floatToInt16(floatSamples) {
    const int16 = new Int16Array(floatSamples.length);
    for (let i = 0; i < floatSamples.length; i++) {
      let s = Math.max(-1, Math.min(1, floatSamples[i]));
      int16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
    }
    return int16;
  }
  async function resampleBuffer(buffer, srcSampleRate) {
    if (srcSampleRate === SAMPLE_RATE) return buffer;

    const offlineCtx = new OfflineAudioContext(1, Math.ceil(buffer.length * SAMPLE_RATE / srcSampleRate), SAMPLE_RATE);
    const audioBuffer = offlineCtx.createBuffer(1, buffer.length, srcSampleRate);
    audioBuffer.copyToChannel(new Float32Array(buffer), 0);

    const source = offlineCtx.createBufferSource();
    source.buffer = audioBuffer;
    source.connect(offlineCtx.destination);
    source.start(0);

    const renderedBuffer = await offlineCtx.startRendering();
    return renderedBuffer.getChannelData(0);
  }

  async function stopVAD() {
    stopped = true; 

    if (processorNode) {
      processorNode.disconnect();
      processorNode.onaudioprocess = null;
      processorNode = null;
    }
    if (audioContext) {
      await audioContext.close();
      audioContext = null;
    }
    if (micStream) {
      micStream.getTracks().forEach(track => track.stop());
      micStream = null;
    }
    if (vadInstancePtr) {
      Module._free_vad(vadInstancePtr);
      vadInstancePtr = null;
    }
    if (audioBufferPtr) {
      Module._free(audioBufferPtr);
      audioBufferPtr = null;
    }

    floatBuffer = [];
    silenceFrames = 0;

    startBtn.disabled = false;
    stopBtn.disabled = true;

    log('VAD stopped');
  }

  async function startVAD() {
    if (!stopped) return; // prevent double start

    stopped = false;
    silenceFrames = 0;
    startBtn.disabled = true;
    stopBtn.disabled = false;

    log('Loading VAD Module...');

    Module = await createModule();

    vadInstancePtr = Module._create_vad();
    if (vadInstancePtr === 0) {
      log('Failed to create VAD instance.');
      return;
    }

    const initStatus = Module._init_vad(vadInstancePtr, 2);
    if (initStatus !== 0) {
      log('Failed to initialize VAD with mode 2');
      return;
    }

    audioBufferPtr = Module._malloc(FRAME_SAMPLES * 2);

    audioContext = new (window.AudioContext || window.webkitAudioContext)();

    try {
      micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const sourceNode = audioContext.createMediaStreamSource(micStream);

      processorNode = audioContext.createScriptProcessor(4096, 1, 1);
      sourceNode.connect(processorNode);
      processorNode.connect(audioContext.destination);
      const micSampleRate = audioContext.sampleRate;
      const micFrameSize = Math.floor(FRAME_SAMPLES * micSampleRate / SAMPLE_RATE);

      processorNode.onaudioprocess = async (event) => {
        if (stopped) return;

        const inputBuffer = event.inputBuffer.getChannelData(0);
        floatBuffer.push(...inputBuffer);

        while (floatBuffer.length >= micFrameSize) {
          const frame = floatBuffer.slice(0, micFrameSize);
          floatBuffer = floatBuffer.slice(micFrameSize);

          const resampledFrame = await resampleBuffer(frame, micSampleRate);
          if (resampledFrame.length < FRAME_SAMPLES) continue;

          const int16Frame = floatToInt16(resampledFrame.slice(0, FRAME_SAMPLES));
          Module.HEAP16.set(int16Frame, audioBufferPtr / 2);

          const result = Module._is_speech(vadInstancePtr, audioBufferPtr, SAMPLE_RATE, FRAME_SAMPLES);

          if (result === 1) {
            if (silenceFrames > 0) log('Speech resumed');
            silenceFrames = 0;
          } else if (result === 0) {
            silenceFrames++;
            log(`Silence frame ${silenceFrames}/${SILENCE_FRAME_COUNT}`);

            if (silenceFrames >= SILENCE_FRAME_COUNT) {
              log('Silence timeout reached. Stopping VAD...');
              await stopVAD();
              return;
            }
          } else {
            log('VAD error');
          }
        }
      };

      log('Audio capture started. Speak now.');
    } catch (err) {
      log('Error accessing microphone: ' + err.message);
      startBtn.disabled = false;
      stopBtn.disabled = true;
    }
  }

  startBtn.onclick = startVAD;
  stopBtn.onclick = stopVAD;

})();
</script>

</body>
</html>
